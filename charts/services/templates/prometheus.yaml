{{- if and .Values.primaryTenant (not .Values.disableAllApplications) .Values.applications.prometheus.enabled }}
# ---
# apiVersion: v1
# kind: Secret
# metadata:
#   name: prometheus
#   namespace: kube-system
# type: Opaque
# stringData:
#   grafana-admin-user: "someuser"
#   grafana-admin-password: "somepass"
#   grafana-gotify-token: "sometoken"

---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: prometheus-operator-crds
  namespace: kube-system
spec:
  chart: prometheus-operator-crds
  repo: https://prometheus-community.github.io/helm-charts
  version: 24.0.1
  targetNamespace: kube-system
  bootstrap: true

---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: prometheus
  namespace: kube-system
spec:
  chart: kube-prometheus-stack
  repo: https://prometheus-community.github.io/helm-charts
  version: 78.5.0
  targetNamespace: kube-system
  valuesContent: |-
    crds:
      enabled: false
    prometheus-node-exporter:
      hostNetwork: false
      prometheus:
        monitor:
          relabelings:
            - sourceLabels:
                - __meta_kubernetes_pod_node_name
              targetLabel: instance
              action: replace
            - sourceLabels: 
                - __meta_kubernetes_pod_node_name
              separator: ;
              regex: ^(.*)$
              targetLabel: nodename
              replacement: $1
              action: replace
          attachMetadata:
            node: true
      resources:
        limits:
          memory: 64Mi
        requests:
          memory: 16Mi
          cpu: 100m
    defaultRules:
      rules:
        windows: false
    grafana:
      annotations:
        reloader.stakater.com/auto: "true"
{{- if .Values.applications.gotify.enabled }}
      envValueFrom:
        GOTIFY_TOKEN:
          secretKeyRef:
            name: prometheus
            key: grafana-gotify-token
      alerting:
        contactpoints.yaml:
          secret:
            apiVersion: 1
            contactPoints:
              - orgId: 1
                name: gotify
                receivers:
                  - uid: gotify-webhook
                    type: webhook
                    disableResolveMessage: false
                    settings:
                      url: "http://gotify.{{ .Values.applicationsNamespace }}.svc.cluster.local/message?token=${GOTIFY_TOKEN}"
                      httpMethod: POST
                      httpHeader:
                        - key: Content-Type
                          value: application/json
{{- end }}
{{- if and (not .Values.disableAllApplications) (or .Values.applications.loki.enabled .Values.applications.tempo.enabled) }}
      additionalDataSources:
{{- if .Values.applications.loki.enabled }}
        - name: Loki
          type: loki
          uid: loki
          access: proxy
          url: http://loki.kube-system.svc.cluster.local:3100
          isDefault: false
          jsonData:
            timeout: 60
            maxLines: 1000
{{- end }}
{{- if .Values.applications.tempo.enabled }}
        - name: Tempo
          uid: tempo
          type: tempo
          access: proxy
          url: http://tempo.kube-system.svc.cluster.local:3200
          isDefault: false
          jsonData:
            tracesToLogsV2:
              datasourceUid: loki
              spanStartTimeShift: -2m
              spanEndTimeShift: 2m
              tags: ['cluster','namespace','pod','service.name','service.namespace']
              customQuery: true
              query: '{ $$__tags } |~ "$${__span.traceId}"'
            tracesToMetrics:
              datasourceUid: prometheus
              spanStartTimeShift: -2m
              spanEndTimeShift: 2m
              tags: [{ key: 'service.name', value: 'service' }, { key: 'service.namespace', value: 'namespace' }]
              queries:
                - name: Latency p95
                  unit: ms
                  query: 'histogram_quantile(0.95, sum by (le,service,span_name) (rate(traces_spanmetrics_latency_bucket{$$__tags}[5m])))'
                - name: Error rate
                  unit: ops
                  query: 'sum by (service,span_name) (rate(traces_spanmetrics_calls_total{$$__tags,status_code="STATUS_CODE_ERROR"}[5m]))'
{{- end }}
{{- end }}
      defaultDashboards:
        namespace: kube-system
        useExistingNamespace: true
      defaultDashboardsTimezone: browser
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
            - name: 'default'
              orgId: 1
              folder: ''
              type: file
              disableDeletion: false
              allowUiUpdates: true
              editable: true
              options:
                path: /var/lib/grafana/dashboards/default
      dashboards:
        default:
          cert-manager:
            gnetId: 20340
            revision: 1
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
          traefik:
            gnetId: 17347
            revision: 9
            datasource: Prometheus
          node-exporter-full:
            gnetId: 1860
            revision: 41
            datasource: Prometheus
          kubernetes-views-global:
            gnetId: 15757
            revision: 1
            datasource: Prometheus
          kubernetes-views-nodes:
            gnetId: 15759
            revision: 37
            datasource: Prometheus
          kubernetes-views-namespaces:
            gnetId: 15758
            revision: 42
            datasource: Prometheus
          prometheus:
            gnetId: 19105
            revision: 7
            datasource: Prometheus
          oom:
            gnetId: 16718
            revision: 1
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
{{- if .Values.applications.argo.enabled }}
          argo-cd:
            gnetId: 14584
            revision: 1
            datasource: Prometheus
{{- end }}
{{- if or .Values.applications.prowlarr.enabled .Values.applications.radarr.enabled .Values.applications.sonarr.enabled }}
          arr:
            url: https://raw.githubusercontent.com/onedr0p/exportarr/3cd394508cf9456f8df4ce4a5d1adc33bb51da2b/examples/grafana/dashboard2.json
            datasource: Prometheus
{{- end }}
{{- if .Values.applications.authentik.enabled }}
          authentik:
            gnetId: 14837
            revision: 2
            datasource: Prometheus
{{- end }}
{{- if .Values.applications.crowdsec.enabled }}
          crowdsec:
            gnetId: 24049
            revision: 1
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
              - name: DS_LOKI
                value: Loki
{{- end }}
{{- if .Values.applications.loki.enabled }}
          loki:
            gnetId: 14055
            revision: 5
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
              - name: DS_LOKI
                value: Loki
{{- end }}
{{- if .Values.applications.postgresql.enabled }}
          pgbackrest:
            url: https://raw.githubusercontent.com/CrunchyData/postgres-operator-examples/6b9bb005f171690d228bf49ca19f8f4caff95451/kustomize/monitoring/grafana/dashboards/pgbackrest.json
            datasource: Prometheus
          postgresql-pod-details:
            url: https://raw.githubusercontent.com/CrunchyData/postgres-operator-examples/6b9bb005f171690d228bf49ca19f8f4caff95451/kustomize/monitoring/grafana/dashboards/pod_details.json
            datasource: Prometheus
          postgresql-details:
            url: https://raw.githubusercontent.com/CrunchyData/postgres-operator-examples/6b9bb005f171690d228bf49ca19f8f4caff95451/kustomize/monitoring/grafana/dashboards/postgresql_details.json
            datasource: Prometheus
          postgresql-service-health:
            url: https://raw.githubusercontent.com/CrunchyData/postgres-operator-examples/6b9bb005f171690d228bf49ca19f8f4caff95451/kustomize/monitoring/grafana/dashboards/postgresql_service_health.json
            datasource: Prometheus
          postgresql-query-statistics:
            url: https://raw.githubusercontent.com/CrunchyData/postgres-operator-examples/6b9bb005f171690d228bf49ca19f8f4caff95451/kustomize/monitoring/grafana/dashboards/query_statistics.json
            datasource: Prometheus
{{- end }}
{{- if .Values.applications.idrac_exporter.enabled }}
          bmc:
            url: https://raw.githubusercontent.com/mrlhansen/idrac_exporter/8d5d7e3da3d98f773dbfcc2709f21b1d3045f845/grafana/idrac.json
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
          bmc-overview:
            url: https://raw.githubusercontent.com/mrlhansen/idrac_exporter/8d5d7e3da3d98f773dbfcc2709f21b1d3045f845/grafana/idrac_overview.json
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
          bmc-status:
            url: https://raw.githubusercontent.com/mrlhansen/idrac_exporter/8d5d7e3da3d98f773dbfcc2709f21b1d3045f845/grafana/status-alternative.json
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
{{- end }}
{{- if .Values.applications.ipmi_exporter.enabled }}
          ipmi:
            gnetId: 15765
            revision: 2
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
{{- end }}
{{- if .Values.applications.minio.enabled }}
          minio:
            gnetId: 13502
            revision: 26
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
{{- end }}
{{- if .Values.applications.node_problem_detector.enabled }}
          node-problem-detector:
            gnetId: 15549
            revision: 1
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
{{- end }}
{{- if .Values.applications.redis.enabled }}
          redis:
            gnetId: 763
            revision: 6
            datasource:
              - name: DS_PROM
                value: Prometheus
          redis-quickstart:
            gnetId: 14091
            revision: 1
            datasource: Prometheus
{{- end }}
{{- if .Values.applications.velero.enabled }}
          velero:
            # gnetId: 23838
            gnetId: 15469
            revision: 1
            datasource: Prometheus
{{- end }}
      prune: true
      sidecar:
        dashboards:
          searchNamespace: ALL
      admin:
        existingSecret: prometheus
        userKey: grafana-admin-user
        passwordKey: grafana-admin-password
      grafana.ini:
        dashboards:
          default_home_dashboard_path: /var/lib/grafana/dashboards/default/kubernetes-views-global.json
        server:
          root_url: https://grafana.{{ .Values.fqdn }}
          domain: grafana.{{ .Values.fqdn }}
          serve_from_sub_path: true
      ingress:
        enabled: true
        annotations:
          homer.service.name: Monitoring
          homer.item.name: Grafana
          homer.item.logo: "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a1/Grafana_logo.svg/1920px-Grafana_logo.svg.png"
          traefik.ingress.kubernetes.io/router.middlewares: kube-system-intranet-allowlist@kubernetescrd
        hosts:
          - grafana.{{ .Values.fqdn }}
        tls:
          - secretName: "{{ .Values.fqdn }}-tls"
            hosts:
              - grafana.{{ .Values.fqdn }}
      persistence:
        enabled: true
        storageClassName: local-path-persistent
        accessModes:
          - ReadWriteOnce
        size: 20Gi
        finalizers:
          - kubernetes.io/pvc-protection
      nodeSelector:
        # Schedule onto amd64 to specifically avoid raspberry pi to not wear the SD card
        kubernetes.io/arch: amd64
      resources:
        limits:
          memory: 2Gi
        requests:
          memory: 1Gi
          cpu: 2000m
    prometheus:
      prometheusSpec:
        podMonitorSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        scrapeConfigSelectorNilUsesHelmValues: false
        ruleSelectorNilUsesHelmValues: false
        replicas: 1
        retention: 365d
        podAntiAffinity: hard
        storageSpec:
          volumeClaimTemplate:
            spec:
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 50Gi
        nodeSelector:
          # Schedule onto amd64 to specifically avoid raspberry pi to not wear the SD card
          kubernetes.io/arch: amd64
        resources:
          limits:
            memory: 3Gi
          requests:
            cpu: 500m
            memory: 1Gi
{{- if .Values.applications.ipmi_exporter.enabled }}
        secrets:
          - ipmi-exporter
{{- end }}
    ## k3s only ##
    # Avoid duplicated metrics in k3s: https://tinyurl.com/mrhjre9n
    kubelet:
      jobNameOverride: k3s-server
      service:
        enabled: false
      serviceMonitor:
        enabled: false
    # Disable community scrapers and let pushprox scrape metrics instead for k3s
    kubeControllerManager:
      jobNameOverride: k3s-server
      service:
        enabled: false
      serviceMonitor:
        enabled: false
    kubeScheduler:
      jobNameOverride: k3s-server
      service:
        enabled: false
      serviceMonitor:
        enabled: false
    kubeProxy:
      jobNameOverride: k3s-server
      service:
        enabled: false
      serviceMonitor:
        enabled: false
    kubeEtcd:
      service:
        enabled: false
      serviceMonitor:
        enabled: false
    extraManifests:
      - apiVersion: helm.cattle.io/v1
        kind: HelmChart
        metadata:
          name: pushprox-k3s-server
          namespace: kube-system
        spec:
          chart: rancher-pushprox
          repo: https://arch-anes.github.io/charts
          version: 0.1.0
          targetNamespace: kube-system
          valuesContent: |-
            metricsPort: 10250
            component: k3s-server
            clients:
              port: 10013
              useLocalhost: true
              resources:
                limits:
                  memory: 64Mi
                requests:
                  memory: 16Mi
                  cpu: 100m
              https:
                enabled: true
                useServiceAccountCredentials: true
                insecureSkipVerify: true
              rbac:
                additionalRules:
                  - nonResourceURLs: ["/metrics/cadvisor"]
                    verbs: ["get"]
                  - apiGroups: [""]
                    resources: ["nodes/metrics"]
                    verbs: ["get"]
              tolerations:
                - effect: "NoExecute"
                  operator: "Exists"
                - effect: "NoSchedule"
                  operator: "Exists"
            serviceMonitor:
              endpoints:
                - port: metrics
                  honorLabels: true
                  relabelings:
                    - sourceLabels: [__metrics_path__]
                      targetLabel: metrics_path
                    - sourceLabels:
                        - __meta_kubernetes_pod_node_name
                      targetLabel: instance
                      action: replace
                - port: metrics
                  path: /metrics/cadvisor
                  honorLabels: true
                  relabelings:
                    - sourceLabels: [__metrics_path__]
                      targetLabel: metrics_path
                    - sourceLabels:
                        - __meta_kubernetes_pod_node_name
                      targetLabel: instance
                      action: replace
                - port: metrics
                  path: /metrics/probes
                  honorLabels: true
                  relabelings:
                    - sourceLabels: [__metrics_path__]
                      targetLabel: metrics_path
                    - sourceLabels:
                        - __meta_kubernetes_pod_node_name
                      targetLabel: instance
                      action: replace
            proxy:
              resources:
                limits:
                  memory: 64Mi
                requests:
                  memory: 16Mi
                  cpu: 100m
      - apiVersion: helm.cattle.io/v1
        kind: HelmChart
        metadata:
          name: pushprox-kube-etcd
          namespace: kube-system
        spec:
          chart: rancher-pushprox
          repo: https://arch-anes.github.io/charts
          version: 0.1.0
          targetNamespace: kube-system
          valuesContent: |-
            metricsPort: 2381
            component: kube-etcd
            clients:
              port: 10014
              useLocalhost: true
              nodeSelector:
                node-role.kubernetes.io/etcd: "true"
              resources:
                limits:
                  memory: 64Mi
                requests:
                  memory: 16Mi
                  cpu: 100m
              tolerations:
                - effect: "NoExecute"
                  operator: "Exists"
                - effect: "NoSchedule"
                  operator: "Exists"
            serviceMonitor:
              endpoints:
                - port: metrics
                  honorLabels: true
                  relabelings:
                    - targetLabel: source
                      replacement: kubernetes
                    - sourceLabels:
                        - __meta_kubernetes_pod_node_name
                      targetLabel: instance
                      action: replace
            proxy:
              resources:
                limits:
                  memory: 64Mi
                requests:
                  memory: 16Mi
                  cpu: 100m
{{- end }}
